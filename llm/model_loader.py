# llm/model_loader.py

import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

from config.config import (
    MODEL_NAME,
    USE_REACT_PLANNER_LORA,
    REACT_LORA_PATH,
)


def load_hf_model():
    """
    HuggingFace LLaMA ê¸°ë°˜ 30B ëª¨ë¸ì„ ë¡œë“œí•˜ê³ ,
    í•„ìš”í•˜ë‹¤ë©´ ReAct Plannerìš© LoRA ì–´ëŒ‘í„°ë¥¼ ë¶™ì—¬ì„œ ë°˜í™˜í•œë‹¤.
    """

    print(f"[ğŸ”„] Loading HuggingFace base model: {MODEL_NAME}")

    # -------------------------------------------------
    # 1) í† í¬ë‚˜ì´ì € ë¡œë”©
    # -------------------------------------------------
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_NAME,
        use_fast=False,
    )

    # pad_token ì—†ìœ¼ë©´ eos_tokenìœ¼ë¡œ ë§ì¶”ê¸° (30B ê³„ì—´ì—ì„œ ìì£¼ í•„ìš”í•œ ì„¤ì •)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    # -------------------------------------------------
    # 2) ë² ì´ìŠ¤ ëª¨ë¸ (fp16, device_map='auto')
    #    - ì—¬ê¸°ì„œëŠ” bitsandbytes ì‚¬ìš© ì•ˆ í•¨ (ìˆœìˆ˜ fp16)
    # -------------------------------------------------
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.float16,
        device_map="auto",
    )

    # generate() ì‹œ warning ì¤„ì´ê¸° ìœ„í•´ pad_token_id ì§€ì •
    if getattr(model.config, "pad_token_id", None) is None:
        model.config.pad_token_id = tokenizer.pad_token_id

    # LoRA + SFT í•™ìŠµ ì‹œì—ëŠ” use_cache=False ê°€ ë” ì•ˆì „í•œ ê²½ìš°ê°€ ë§ìŒ
    if hasattr(model, "config"):
        model.config.use_cache = False

    print("[âœ…] Base model loaded (fp16, no quantization)")

    # -------------------------------------------------
    # 3) ReAct Planner LoRA ì–´ëŒ‘í„° ë¡œë”© (ì„ íƒì )
    # -------------------------------------------------
    if USE_REACT_PLANNER_LORA:
        print(f"[ğŸ”] Trying to load ReAct LoRA from: {REACT_LORA_PATH}")
        if os.path.isdir(REACT_LORA_PATH):
            try:
                model = PeftModel.from_pretrained(
                    model,
                    REACT_LORA_PATH,
                    # torch_dtype=torch.float16,  # ì´ë¯¸ baseê°€ fp16 ì´ë¯€ë¡œ ìƒëµ ê°€ëŠ¥
                )
                print("[âœ…] ReAct Planner LoRA attached successfully.")
            except Exception as e:
                print(f"[âš ] Failed to load LoRA adapter: {e}")
                print("    â†’ LoRA ì—†ì´ base ëª¨ë¸ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        else:
            print(f"[âš ] LoRA path not found: {REACT_LORA_PATH}")
            print("    â†’ USE_REACT_PLANNER_LORA=True ì´ì§€ë§Œ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. base ëª¨ë¸ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    else:
        print("[â„¹] USE_REACT_PLANNER_LORA=False â†’ LoRA ë¯¸ì ìš©, base ëª¨ë¸ë§Œ ì‚¬ìš©.")

    return tokenizer, model
