# llm/model_loader.py

import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

from config.config import (
    MODEL_NAME,
    USE_REACT_PLANNER_LORA,
    REACT_LORA_PATH,
)

def load_hf_model():
    """
    HuggingFace LLaMA ê¸°ë°˜ 30B ëª¨ë¸ì„ ë¡œë“œí•˜ê³ ,
    í•„ìš”í•˜ë‹¤ë©´ ReAct Plannerìš© LoRA ì–´ëŒ‘í„°ë¥¼ ë¶™ì—¬ì„œ ë°˜í™˜í•œë‹¤.
    """

    print(f"[ğŸ”„] Loading HuggingFace base model: {MODEL_NAME}")

    # -------------------------------------------------
    # 1) í† í¬ë‚˜ì´ì € ë¡œë”©
    # -------------------------------------------------
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_NAME,
        use_fast=False,
    )

    # pad_token ì—†ìœ¼ë©´ eos_tokenìœ¼ë¡œ ë§ì¶”ê¸° (30B ê³„ì—´ì—ì„œ ìì£¼ í•„ìš”í•œ ì„¤ì •)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    # -------------------------------------------------
    # 2) ë² ì´ìŠ¤ ëª¨ë¸ (fp16, device_map='auto')
    #    - ì—¬ê¸°ì„œëŠ” bitsandbytes ì‚¬ìš© ì•ˆ í•¨ (ìˆœìˆ˜ fp16)
    # -------------------------------------------------
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.float16,
        device_map="auto",
    )

    # generate() ì‹œ warning ì¤„ì´ê¸° ìœ„í•´ pad_token_id ì§€ì •
    if getattr(model.config, "pad_token_id", None) is None:
        model.config.pad_token_id = tokenizer.pad_token_id

    # LoRA + SFT í•™ìŠµ ì‹œì—ëŠ” use_cache=False ê°€ ë” ì•ˆì „í•œ ê²½ìš°ê°€ ë§ìŒ
    if hasattr(model, "config"):
        model.config.use_cache = False

    print("[âœ…] Base model loaded (fp16, no quantization)")