# llm/model_loader.py

import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

from config.config import (
    MODEL_NAME,
    USE_REACT_PLANNER_LORA,
    REACT_LORA_PATH,
)

def load_hf_model():
    """
    HuggingFace LLaMA ê¸°ë°˜ 30B ëª¨ë¸ì„ ë¡œë“œí•˜ê³ ,
    í•„ìš”í•˜ë‹¤ë©´ ReAct Plannerìš© LoRA ì–´ëŒ‘í„°ë¥¼ ë¶™ì—¬ì„œ ë°˜í™˜í•œë‹¤.
    """

    print(f"[ğŸ”„] Loading HuggingFace base model: {MODEL_NAME}")

    # -------------------------------------------------
    # 1) í† í¬ë‚˜ì´ì € ë¡œë”©
    # -------------------------------------------------
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_NAME,
        use_fast=False,
    )

    # pad_token ì—†ìœ¼ë©´ eos_tokenìœ¼ë¡œ ë§ì¶”ê¸° (30B ê³„ì—´ì—ì„œ ìì£¼ í•„ìš”í•œ ì„¤ì •)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"