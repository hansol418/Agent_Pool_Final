# test_react_planner_only.py
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from config.paths import REACT_LORA_DIR
from config.config import BASE_MODEL_NAME


LORA_PATH = REACT_LORA_DIR.as_posix()

# âœ… ì—¬ê¸° í…œí”Œë¦¿ì€ make_react_dataset.py ì˜ BASE_PROMPT_TEMPLATE ì™€ ì™„ì „íˆ ë™ì¼í•˜ê²Œ ë§ì¶°ì•¼ í•¨
BASE_PROMPT_TEMPLATE = """ë‹¹ì‹ ì€ ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í•œêµ­ì–´ ì§€ëŠ¥í˜• ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.

ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬:
- web_search : ì›¹ ê²€ìƒ‰ì„ í†µí•´ ìµœì‹  ì •ë³´, ì¼ë°˜ ìƒì‹, ìƒí™œ ì •ë³´ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
- doc_search : ë‚´ë¶€ ë¬¸ì„œ(AI ê°œë…, ìì—°ì–´ ì²˜ë¦¬, ê°•í™”í•™ìŠµ ë“±)ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
- summarize  : ì—¬ëŸ¬ ì •ë³´ë¥¼ ìš”ì•½í•˜ì—¬ ì •ë¦¬í•©ë‹ˆë‹¤.

ì¶œë ¥ í˜•ì‹(ë°˜ë“œì‹œ ì´ 3ì¤„ë§Œ í¬í•¨):

Thought: (ì§§ì€ ìƒê°)
Action: web_search | doc_search | summarize | FINAL
Action Input: (ë„êµ¬ì— ë„˜ê¸¸ ì…ë ¥ ë‚´ìš© ë˜ëŠ” FINALì¼ ê²½ìš° ìµœì¢… ë‹µë³€ ì „ì²´)

ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™” ë° ë„êµ¬ ê²°ê³¼:
{history_block}

ì‚¬ìš©ìì˜ ì§ˆë¬¸:
{user_query}

ìœ„ í˜•ì‹ì„ ë”°ë¼ ë‹¤ìŒ í–‰ë™ì„ ê²°ì •í•˜ì„¸ìš”.
"""


def build_prompt(user_query: str, history_block: str = "ì—†ìŒ") -> str:
    return BASE_PROMPT_TEMPLATE.format(
        history_block=history_block,
        user_query=user_query,
    )


def main():
    print("[ğŸ”„] Loading base model + LoRA (ReAct planner)...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, use_fast=False)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ (fp16)
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_NAME,
        torch_dtype=torch.float16,
        device_map="auto",
    )

    # ReAct LoRA ì–´ëŒ‘í„° ë¡œë“œ
    model = PeftModel.from_pretrained(model, LORA_PATH)
    model.eval()

    # âœ… í…ŒìŠ¤íŠ¸ìš© ì§ˆë¬¸ (doc_search, web_search, FINAL ë‹¤ ì—¬ëŸ¬ ê°œ ì‹œë„í•´ë´ë„ ì¢‹ìŒ)
    user_query = "ë„ˆëŠ” ì–´ë–¤ ì—­í• ì„ í•˜ëŠ” ì—ì´ì „íŠ¸ì•¼?"
    prompt = build_prompt(user_query)

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=128,
            do_sample=False,   # ìš°ì„  greedyë¡œ í˜•ì‹ ì—¬ë¶€ë§Œ í™•ì¸
        )

    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print("===== FULL OUTPUT =====")
    print(full_text)

    # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì˜ë¼ë‚´ê³ , ëª¨ë¸ì´ ìƒˆë¡œ ìƒì„±í•œ ë¶€ë¶„ë§Œ ë³´ê³  ì‹¶ìœ¼ë©´:
    print("\n===== GENERATED PART ONLY =====")
    print(full_text[len(prompt):])


if __name__ == "__main__":
    main()
