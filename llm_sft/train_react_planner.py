# train_react_planner.py
# ReAct Planner SFT (LoRA, bf16 / no bitsandbytes)

import os
from dataclasses import dataclass
from typing import Dict, Optional, List
from config.paths import REACT_DATA_JSONL, REACT_LORA_OUT
from config.config import BASE_MODEL_NAME

import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
)
from peft import LoraConfig
from trl import SFTTrainer


# -------------------------------------------------------
# 0) ê¸°ë³¸ ì„¤ì •
# -------------------------------------------------------

# í•™ìŠµ ë°ì´í„° íŒŒì¼ (2ë‹¨ê³„ì—ì„œ ë§Œë“  JSONL)
DATA_PATH = REACT_DATA_JSONL.as_posix()

# LoRA ê²°ê³¼ ì €ì¥ ê²½ë¡œ
OUTPUT_DIR = REACT_LORA_OUT.as_posix()


# -------------------------------------------------------
# 1) Dataset ë¡œë“œ
#   - JSONL: {"prompt": "...", "response": "..."} êµ¬ì¡°
# -------------------------------------------------------

def load_react_dataset(path: str):
    """
    react_sft_data.jsonl ì„ datasets í¬ë§·ìœ¼ë¡œ ë¡œë“œ.
    """
    dataset = load_dataset("json", data_files=path)
    # dataset["train"] í•˜ë‚˜ë§Œ ì“°ë©´ ì¶©ë¶„ (train / test ë‚˜ëˆ„ê³  ì‹¶ìœ¼ë©´ split ê°€ëŠ¥)
    return dataset["train"]


# -------------------------------------------------------
# 2) í…ìŠ¤íŠ¸ í¬ë§·íŒ… í•¨ìˆ˜
#    - prompt + response ë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹œë‹¤.
#    - ì¶”ë¡  ë•ŒëŠ” "promptë§Œ ë„£ê³  -> ëª¨ë¸ì´ response(3ì¤„)ë¥¼ ì´ì–´ì„œ ìƒì„±"í•˜ê²Œ ì‚¬ìš©.
# -------------------------------------------------------

def formatting_func(batch: Dict[str, List[str]]) -> List[str]:
    """
    batch["prompt"], batch["response"] ë¥¼ ë°›ì•„ì„œ
    "prompt + response" í˜•íƒœì˜ ë‹¨ì¼ í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¡œ í•©ì¹œë‹¤.
    """
    outputs = []
    prompts = batch["prompt"]
    responses = batch["response"]

    for p, r in zip(prompts, responses):
        # ì¤‘ê°„ì— êµ¬ë¶„ ì¤„ í•˜ë‚˜ ì •ë„ë§Œ ë„£ê³  ë°”ë¡œ responseë¥¼ ë¶™ì¸ë‹¤.
        text = p.rstrip() + "\n" + r.strip()
        outputs.append(text)

    return outputs


# -------------------------------------------------------
# 3) ëª¨ë¸ / í† í¬ë‚˜ì´ì € ë¡œë“œ (bf16 LoRA, no bitsandbytes)
# -------------------------------------------------------

def load_model_and_tokenizer():
    print(f"[ğŸ”„] Loading base model (bf16, no bitsandbytes): {BASE_MODEL_NAME}")

    # 3-1) í† í¬ë‚˜ì´ì €
    tokenizer = AutoTokenizer.from_pretrained(
        BASE_MODEL_NAME,
        use_fast=False,
    )

    # pad_token ì—†ìœ¼ë©´ eos_tokenìœ¼ë¡œ ë§ì¶°ì£¼ê¸°
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # SFTì—ì„œëŠ” ì¼ë°˜ì ìœ¼ë¡œ right padding ì‚¬ìš©
    tokenizer.padding_side = "right"

    # 3-2) ëª¨ë¸ ë¡œë“œ (bfloat16, ì–‘ìí™” ì—†ìŒ)
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_NAME,
        torch_dtype=torch.bfloat16,   # âœ… bf16
        device_map="auto",            # ì—¬ëŸ¬ GPU ìˆìœ¼ë©´ ìë™ ë¶„ì‚°
    )

    # LoRA í•™ìŠµ ì‹œì—ëŠ” use_cache=False ë¡œ ë‘ëŠ” ê²Œ ì•ˆì •ì 
    if hasattr(model, "config"):
        model.config.use_cache = False

    print("[âœ…] Model & tokenizer loaded (bf16, no quantization)")
    return model, tokenizer


def main():
    train_dataset = load_react_dataset(REACT_DATA_JSONL.as_posix())
    model, tokenizer = load_model_and_tokenizer()


if __name__ == "__main__":
    main()